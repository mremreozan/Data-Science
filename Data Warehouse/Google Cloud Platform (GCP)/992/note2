1-
I am going to talk about Hadoop. Hadoop 
is used in Data Science. The purpose of 
this presentation is that it should at 
least make sense to us and we will be 
less confused when people talk about it. 
Because, Understanding Hadoop is a highly 
valuable skill for anyone working at 
companies with large amounts of data.
2-
Hadoop is used for storing big data and 
performing operations on data. Azure 
HDInsight is a cloud distribution of Hadoop 
components. Amazon Elastic MapReduce (EMR) 
is an Amazon Web Services tool for big data 
processing and analysis. Google Cloud 
Dataproc, It can be used for big data 
processing and machine learning. 
And there are a lot of tools using hadoop 
by integrating some another technology.
3- 
Hadoop is a synonym of Big data. Big Data 
comprises of 3 V s or characteristics: 
volume, velocity, and variety. Hadoop 
framework plays a leading role in storing 
and processing Big Data.
4-
Hadoop is an open source software platform 
having distributed storage and distributed 
processing of very large data sets on computer 
clusters. The important components or concepts 
in Hadoop is MapReduce, HDFS, and YARN. 
It has many similarities with existing 
distributed file systems. 
5-
However, the differences from other 
distributed file systems are significant. 
1) HDFS is highly fault-tolerant and is 
designed to be deployed on low-cost hardware. 
2-3) HDFS provides high access to application 
data and is suitable for applications that 
have large data sets. 4) HDFS relaxes a few 
POSIX requirements to enable streaming access 
to file system data. 5) The assumption is that 
it is often better to migrate the computation 
closer to where the data is located rather 
than moving the data to where the application 
is running. 
6- 
First of all, let us understand why you need 
it and what the alternative is. When you have 
an useful 8 GB data. There are two options 
to store it. The first one is to get a big 
capacity node. This approach is called scale up, 
or vertical scaling. Do you want to store 
more data? Get yourself a better, bigger. 
The second one is to store our data on a 
collection of nodes. This approach called 
Scale Out or horizontal Scale In. This is 
the style of distributed file system.
7-
We only need to understand the main concepts 
such as Namenode, Datanode, their role and 
level of responsibility between them.
HDFS has a master/slave architecture. 
An HDFS cluster consists of a single NameNode. 
Namenode is  a master server that manages 
the file system namespace and regulates access 
to files by clients. In addition, there are 
a number of DataNodes, usually one per node 
in the cluster, which manage storage attached 
to the nodes that they run on. HDFS Client is 
our command line interface like a terminal in 
Linux. For example, if we want to transfer a 
file from local to hdfs, we use HDFS CLient. 
HDFS client requests a lease from a name node 
to have an exclusive access to write or append 
data to a file. The DataNodes are responsible 
for serving read and write requests from the 
file system s clients. The DataNodes also 
perform block creation, deletion, and replication 
upon instruction from the NameNode. 
8-
A key advantage of using Hadoop is its fault 
tolerance. When data is sent to an individual 
node, that data is also replicated to other 
nodes in the cluster, which means that in the 
event of failure, there is another copy available 
for use. Let me introduce a few concepts.
Block and replica. Replica is a physical data 
storage on a data node. There are usually 
several replicas with the same content on 
different data nodes. Block is a meta-information 
storage on a name node and provides information 
about replica's locations and their states. 
In case of HDFS client dies, replicas will 
participate in a special recovery process called 
a lease recovery.
9-
You will be able to configure HDFS storage for 
your own purposes. Think about the following 
question, is it a big difference to have two terabyte 
hard drive per node or two hard drives per node. 
For instance, you have Samsung 940 Pro SSD with 
disk read speed of three and a half gigabytes 
per second. Let us evaluate how long it takes 
to read 10 petabytes of data from a hard drive 
with similar reading speed. Approximately, 
you will see 35 days to read these data from one disk. 
But if you have 2 drives, it is half of that, 17.5 days.
10-
Let’s us examine an example to understand map reduce. 
There are three datasets which include all of texts 
of the books in these library. For example First 
dataset include all text of all books in the Royal 
library of Belgium. We want to find the count of 
“Champs-Élysées” word in the datasets. In mapreduce 
with Hadoop, Firstly, we will find the count of 
“Champs-Élysées” in each dataset in different servers. 
And then we add up the results by reduce.  What's the 
other way? Or what is before mapreduce of hadoop? 
After combining of datasets, we can map it. But 
if we want to count all words in all library of worlds. 
We would need a big storage and big process capacity.
11-
These are all the other systems around hadoop.
12-
Basically, that’s all I have to say about Hadoop. 
Thank you for listening.
