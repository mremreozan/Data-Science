1-
I am going to talk about Hadoop. Hadoop 
is used in Data Science. The purpose of 
this presentation is that it should at 
least make sense to us and we will be 
less confused when people talk about it. 
Because, Understanding Hadoop is a highly 
valuable skill for anyone working at 
companies with large amounts of data.
2-
Hadoop is used for storing big data and 
performing operations on data. Azure 
HDInsight is a cloud distribution of Hadoop 
components. Amazon Elastic MapReduce (EMR) 
is an Amazon Web Services tool for big data 
processing and analysis. Google Cloud 
Dataproc, It can be used for big data 
processing and machine learning. 
And there are a lot of tools using hadoop 
by integrating some another technology.
3- 
Hadoop is a synonym of Big data. Big Data 
comprises of 3 V's or characteristics: 
volume, velocity, and variety. Hadoop 
framework plays a leading role in storing 
and processing Big Data.
4-
Hadoop is an open source software platform 
having distributed storage and distributed 
processing of very large data sets on computer 
clusters. The important components or concepts 
in Hadoop is MapReduce, HDFS, and YARN. 
It has many similarities with existing 
distributed file systems. 
5-
However, the differences from other 
distributed file systems are significant. 
1) HDFS is highly fault-tolerant and is 
designed to be deployed on low-cost hardware. 
2-3) HDFS provides high access to application 
data and is suitable for applications that 
have large data sets. 4) HDFS relaxes a few 
POSIX requirements to enable streaming access 
to file system data. 5) The assumption is that 
it is often better to migrate the computation 
closer to where the data is located rather 
than moving the data to where the application 
is running. 
6- 
First of all, let us understand why you need 
it and what the alternative is. When you have 
an useful 8 GB data. There are two options 
to store it. The first one is to get a big 
capacity node. This approach is called scale up, 
or vertical scaling. Do you want to store 
more data? Get yourself a better, bigger. 
The second one is to store our data on a 
collection of nodes. This approach called 
Scale Out or horizontal Scale In. This is 
the style of distributed file system.
7-
We only need to understand the main concepts 
such as Namenode, Datanode, their role and 
level of responsibility between them.
HDFS has a master/slave architecture. 
An HDFS cluster consists of a single NameNode. 
Namenode is  a master server that manages 
the file system namespace and regulates access 
to files by clients. In addition, there are 
a number of DataNodes, usually one per node 
in the cluster, which manage storage attached 
to the nodes that they run on. HDFS Client is 
our command line interface like a terminal in 
Linux. For example, if we want to transfer a 
file from local to hdfs, we use HDFS CLient. 
HDFS client requests a lease from a name node 
to have an exclusive access to write or append 
data to a file. The DataNodes are responsible 
for serving read and write requests from the 
file system’s clients. The DataNodes also 
perform block creation, deletion, and replication 
upon instruction from the NameNode. 
8-
A key advantage of using Hadoop is its fault 
tolerance. When data is sent to an individual 
node, that data is also replicated to other 
nodes in the cluster, which means that in the 
event of failure, there is another copy available 
for use. Let me introduce a few concepts.
Block and replica. Replica is a physical data 
storage on a data node. There are usually 
several replicas with the same content on 
different data nodes. Block is a meta-information 
storage on a name node and provides information 
about replica's locations and their states. 
In case of HDFS client dies, replicas will 
participate in a special recovery process called 
a lease recovery.
9-
The goal is to learn from data for which 
the right output is known, so that we can 
make predictions on new data for which we 
don’t know the output.
10-
I will present an example problem to apply a 
machine learning algorithm. It's the application 
for credit card. Commercial banks receive a lot of 
applications for credit cards. 
Many of them get rejected for many reasons, 
like high loan balances, low income levels, 
or too many inquiries on an individual's credit 
report. 
In this project, 
we will build an automatic credit card approval 
predictor using machine learning techniques, 
just like the real banks do.
11-
You see the columns like numbers but they
represent The probable features in a typical 
credit card application are Gender, Age, 
Debt, Married, Income, EducationLevel, 
YearsEmployed, etc, and finally the 
the final variable is ApprovalStatus.  
Zero means rejected, One means accepted.
12-
There are two new application and 
we will predict the approval result.
13-
Machine learning, that’s it. We achieved 
our goal. Here is exciting and the best 
important part, but There are part before 
and after here. Unfortunately they are 
more boring than here.
14- 
We just made the part inside the round. 
Before prediction part, there is a processing 
data including data cleaning, normalization, 
and Split the data into train and test sets. 
And then after prediction part, there is a 
evaluating performance part. 
We check the accuracy of our prediction.
15- 
Thank you for listening.



  















